{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","%cd gdrive/MyDrive/Remi/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xwl9D2Mb5OPH","executionInfo":{"status":"ok","timestamp":1663844260348,"user_tz":-120,"elapsed":17111,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"fe178803-79ce-4869-85b6-38935dbf0601"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/.shortcut-targets-by-id/1WJLnwO7mUn1QFbhFIJ6fMZjZWuCKf1ub/Remi\n"]}]},{"cell_type":"code","source":["#!pip install --upgrade tensorflow-gpu\n","#Install TF-Hub.\n","!pip install tensorflow-hub\n","!pip install seaborn\n","\n","!pip3 install tensorflow-text --force-reinstall\n","import tensorflow_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YGYuYi7Y-GSK","executionInfo":{"status":"ok","timestamp":1663844400484,"user_tz":-120,"elapsed":140153,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"6f5fb880-0d0f-41eb-b8d7-2c1b3fb38e2f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.6)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.7.3)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text\n","  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[K     |████████████████████████████████| 5.9 MB 4.6 MB/s \n","\u001b[?25hCollecting tensorflow-hub>=0.8.0\n","  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 49.6 MB/s \n","\u001b[?25hCollecting tensorflow<2.11,>=2.10.0\n","  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","\u001b[K     |████████████████████████████████| 578.0 MB 16 kB/s \n","\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n","  Downloading grpcio-1.49.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 37.1 MB/s \n","\u001b[?25hCollecting numpy>=1.20\n","  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 33.9 MB/s \n","\u001b[?25hCollecting absl-py>=1.0.0\n","  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 53.1 MB/s \n","\u001b[?25hCollecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n","\u001b[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n","Collecting gast<=0.4.0,>=0.2.1\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting astunparse>=1.6.0\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting setuptools\n","  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 47.6 MB/s \n","\u001b[?25hCollecting typing-extensions>=3.6.6\n","  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Collecting libclang>=13.0.0\n","  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n","\u001b[K     |████████████████████████████████| 14.1 MB 40.5 MB/s \n","\u001b[?25hCollecting google-pasta>=0.1.1\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 6.0 MB/s \n","\u001b[?25hCollecting packaging\n","  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 6.4 MB/s \n","\u001b[?25hCollecting h5py>=2.9.0\n","  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n","\u001b[K     |████████████████████████████████| 4.1 MB 39.8 MB/s \n","\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 37.5 MB/s \n","\u001b[?25hCollecting keras<2.11,>=2.10.0\n","  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 43.9 MB/s \n","\u001b[?25hCollecting six>=1.12.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting opt-einsum>=2.3.2\n","  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 4.7 MB/s \n","\u001b[?25hCollecting flatbuffers>=2.0\n","  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n","Collecting tensorboard<2.11,>=2.10\n","  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n","\u001b[K     |████████████████████████████████| 5.9 MB 40.2 MB/s \n","\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[K     |████████████████████████████████| 438 kB 52.8 MB/s \n","\u001b[?25hCollecting wrapt>=1.11.0\n","  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 4.0 MB/s \n","\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 41.8 MB/s \n","\u001b[?25hCollecting wheel<1.0,>=0.23.0\n","  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 32.8 MB/s \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s \n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[K     |████████████████████████████████| 781 kB 52.1 MB/s \n","\u001b[?25hCollecting google-auth<3,>=1.6.3\n","  Downloading google_auth-2.11.1-py2.py3-none-any.whl (167 kB)\n","\u001b[K     |████████████████████████████████| 167 kB 42.2 MB/s \n","\u001b[?25hCollecting werkzeug>=1.0.1\n","  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n","\u001b[K     |████████████████████████████████| 232 kB 54.5 MB/s \n","\u001b[?25hCollecting requests<3,>=2.21.0\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n","\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n","  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n","Collecting rsa<5,>=3.1.4\n","  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","\u001b[K     |████████████████████████████████| 155 kB 56.1 MB/s \n","\u001b[?25hCollecting requests-oauthlib>=0.7.0\n","  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4\n","  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n","Collecting pyasn1<0.5.0,>=0.4.6\n","  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n","\u001b[?25hCollecting idna<4,>=2.5\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 116 kB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 55.6 MB/s \n","\u001b[?25hCollecting charset-normalizer<3,>=2\n","  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n","Collecting certifi>=2017.4.17\n","  Downloading certifi-2022.9.14-py3-none-any.whl (162 kB)\n","\u001b[K     |████████████████████████████████| 162 kB 56.1 MB/s \n","\u001b[?25hCollecting oauthlib>=3.0.0\n","  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 54.0 MB/s \n","\u001b[?25hCollecting MarkupSafe>=2.1.1\n","  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Collecting pyparsing!=3.0.5,>=2.0.2\n","  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","\u001b[K     |████████████████████████████████| 98 kB 8.9 MB/s \n","\u001b[?25hInstalling collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-hub, tensorflow, tensorflow-text\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: pyasn1\n","    Found existing installation: pyasn1 0.4.8\n","    Uninstalling pyasn1-0.4.8:\n","      Successfully uninstalled pyasn1-0.4.8\n","  Attempting uninstall: idna\n","    Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 2.1.1\n","    Uninstalling charset-normalizer-2.1.1:\n","      Successfully uninstalled charset-normalizer-2.1.1\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2022.6.15\n","    Uninstalling certifi-2022.6.15:\n","      Successfully uninstalled certifi-2022.6.15\n","  Attempting uninstall: zipp\n","    Found existing installation: zipp 3.8.1\n","    Uninstalling zipp-3.8.1:\n","      Successfully uninstalled zipp-3.8.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.1.1\n","    Uninstalling typing-extensions-4.1.1:\n","      Successfully uninstalled typing-extensions-4.1.1\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: rsa\n","    Found existing installation: rsa 4.9\n","    Uninstalling rsa-4.9:\n","      Successfully uninstalled rsa-4.9\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyasn1-modules\n","    Found existing installation: pyasn1-modules 0.2.8\n","    Uninstalling pyasn1-modules-0.2.8:\n","      Successfully uninstalled pyasn1-modules-0.2.8\n","  Attempting uninstall: oauthlib\n","    Found existing installation: oauthlib 3.2.0\n","    Uninstalling oauthlib-3.2.0:\n","      Successfully uninstalled oauthlib-3.2.0\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 4.2.4\n","    Uninstalling cachetools-4.2.4:\n","      Successfully uninstalled cachetools-4.2.4\n","  Attempting uninstall: requests-oauthlib\n","    Found existing installation: requests-oauthlib 1.3.1\n","    Uninstalling requests-oauthlib-1.3.1:\n","      Successfully uninstalled requests-oauthlib-1.3.1\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.0.1\n","    Uninstalling MarkupSafe-2.0.1:\n","      Successfully uninstalled MarkupSafe-2.0.1\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.12.0\n","    Uninstalling importlib-metadata-4.12.0:\n","      Successfully uninstalled importlib-metadata-4.12.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 1.35.0\n","    Uninstalling google-auth-1.35.0:\n","      Successfully uninstalled google-auth-1.35.0\n","  Attempting uninstall: wheel\n","    Found existing installation: wheel 0.37.1\n","    Uninstalling wheel-0.37.1:\n","      Successfully uninstalled wheel-0.37.1\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 1.0.1\n","    Uninstalling Werkzeug-1.0.1:\n","      Successfully uninstalled Werkzeug-1.0.1\n","  Attempting uninstall: tensorboard-plugin-wit\n","    Found existing installation: tensorboard-plugin-wit 1.8.1\n","    Uninstalling tensorboard-plugin-wit-1.8.1:\n","      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.6.1\n","    Uninstalling tensorboard-data-server-0.6.1:\n","      Successfully uninstalled tensorboard-data-server-0.6.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.9\n","    Uninstalling pyparsing-3.0.9:\n","      Successfully uninstalled pyparsing-3.0.9\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: markdown\n","    Found existing installation: Markdown 3.4.1\n","    Uninstalling Markdown-3.4.1:\n","      Successfully uninstalled Markdown-3.4.1\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.48.1\n","    Uninstalling grpcio-1.48.1:\n","      Successfully uninstalled grpcio-1.48.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.2.0\n","    Uninstalling absl-py-1.2.0:\n","      Successfully uninstalled absl-py-1.2.0\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 1.1.0\n","    Uninstalling termcolor-1.1.0:\n","      Successfully uninstalled termcolor-1.1.0\n","  Attempting uninstall: tensorflow-io-gcs-filesystem\n","    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\n","    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\n","      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 21.3\n","    Uninstalling packaging-21.3:\n","      Successfully uninstalled packaging-21.3\n","  Attempting uninstall: opt-einsum\n","    Found existing installation: opt-einsum 3.3.0\n","    Uninstalling opt-einsum-3.3.0:\n","      Successfully uninstalled opt-einsum-3.3.0\n","  Attempting uninstall: libclang\n","    Found existing installation: libclang 14.0.6\n","    Uninstalling libclang-14.0.6:\n","      Successfully uninstalled libclang-14.0.6\n","  Attempting uninstall: keras-preprocessing\n","    Found existing installation: Keras-Preprocessing 1.1.2\n","    Uninstalling Keras-Preprocessing-1.1.2:\n","      Successfully uninstalled Keras-Preprocessing-1.1.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: google-pasta\n","    Found existing installation: google-pasta 0.2.0\n","    Uninstalling google-pasta-0.2.0:\n","      Successfully uninstalled google-pasta-0.2.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0.7\n","    Uninstalling flatbuffers-2.0.7:\n","      Successfully uninstalled flatbuffers-2.0.7\n","  Attempting uninstall: astunparse\n","    Found existing installation: astunparse 1.6.3\n","    Uninstalling astunparse-1.6.3:\n","      Successfully uninstalled astunparse-1.6.3\n","  Attempting uninstall: tensorflow-hub\n","    Found existing installation: tensorflow-hub 0.12.0\n","    Uninstalling tensorflow-hub-0.12.0:\n","      Successfully uninstalled tensorflow-hub-0.12.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","thinc 8.1.0 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","numba 0.56.2 requires setuptools<60, but you have setuptools 65.3.0 which is incompatible.\n","google-api-core 1.31.6 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.11.1 which is incompatible.\n","flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.2.2 which is incompatible.\u001b[0m\n","Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.9.14 charset-normalizer-2.1.1 flatbuffers-2.0.7 gast-0.4.0 google-auth-2.11.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.49.1 h5py-3.7.0 idna-3.4 importlib-metadata-4.12.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.21.6 oauthlib-3.2.1 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 setuptools-65.3.0 six-1.16.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-text-2.10.0 termcolor-2.0.1 typing-extensions-4.3.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","google","numpy","pkg_resources","six"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t77cmo_R1h_k","executionInfo":{"status":"error","timestamp":1663844429973,"user_tz":-120,"elapsed":29509,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"29b154cb-0a17-4acf-8671-8146d78b6147"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scispacy\n","  Downloading scispacy-0.5.1-py3-none-any.whl (44 kB)\n","\u001b[K     |████████████████████████████████| 44 kB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scispacy) (1.21.6)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scispacy) (2.28.1)\n","Collecting pysbd\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[K     |████████████████████████████████| 71 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from scispacy) (1.0.2)\n","Collecting conllu\n","  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n","Collecting nmslib>=1.7.3.6\n","  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n","\u001b[K     |████████████████████████████████| 13.5 MB 24.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from scispacy) (3.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scispacy) (1.1.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n","Collecting pybind11<2.6.2\n","  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n","\u001b[K     |████████████████████████████████| 188 kB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.14)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.26.12)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (4.64.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (21.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.4.4)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.3.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.7)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.9.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.10.1)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.4.2)\n","Collecting typing-extensions<4.2.0,>=3.7.4\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.10)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (8.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->scispacy) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->scispacy) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->scispacy) (5.2.1)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->scispacy) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->scispacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->scispacy) (2.1.1)\n","Installing collected packages: typing-extensions, pybind11, pysbd, nmslib, conllu, scispacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.3.0\n","    Uninstalling typing-extensions-4.3.0:\n","      Successfully uninstalled typing-extensions-4.3.0\n","Successfully installed conllu-4.5.2 nmslib-2.1.1 pybind11-2.6.1 pysbd-0.3.4 scispacy-0.5.1 typing-extensions-4.1.1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-112a4ebf608b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscispacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplain\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_config\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_model\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_diff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_diff\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/cli/debug_diff.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebug_cli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_config_overrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minit_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/cli/init_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jinja2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileSystemBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbccache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemcachedBytecodeCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemplateAssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBLOCK_END_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBLOCK_START_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOMMENT_END_STRING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jinja2/defaults.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFILTERS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDEFAULT_FILTERS\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTESTS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDEFAULT_TESTS\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jinja2/filters.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmarkupsafe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoft_unicode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'soft_unicode' from 'markupsafe' (/usr/local/lib/python3.7/dist-packages/markupsafe/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# !pip install dask\n","# !pip install dask[dataframe] \n","!pip install scispacy\n","\n","import numpy as np\n","import torch\n","import tensorflow\n","import pandas as pd\n","import os\n","import json\n","import time\n","import glob\n","import re\n","import sys\n","import collections\n","from nltk import flatten\n","# import dask\n","# from dask import delayed,compute\n","# import dask.dataframe as dd\n","# from dask.multiprocessing import get\n","from tqdm._tqdm_notebook import tqdm_notebook\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","tqdm_notebook.pandas()\n","from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","\n","import nltk\n","nltk.download('punkt')\n","import scispacy\n","import spacy\n","import scipy"]},{"cell_type":"markdown","metadata":{"id":"gthZYLmr1h_m"},"source":["## Data loading and pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KtWJ7_G1h_m","executionInfo":{"status":"ok","timestamp":1663845895932,"user_tz":-120,"elapsed":10725,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0898317d-964e-45a6-cfe5-c33689444822"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self[name] = value\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["----------\n"]}],"source":["import re\n","import math\n","\n","i = 0\n","dfs = []\n","dir = 'wiki_annotation_pages/nl/'\n","for filename in os.listdir(dir):\n","    data = pd.read_json(dir + filename)  \n","    data = data[\"query\"][\"pages\"][0]\n","    filename = filename.replace('.json', '')\n","    data.update({'filename': filename})\n","    data = pd.DataFrame([data])\n","    dfs.append(data)\n","\n","temp = pd.concat(dfs, ignore_index=True)\n","temp.rename(columns={'title':'Subject'}, inplace=True)\n","temp.rename(columns={'extract':'content'}, inplace=True)\n","\n","df_wikipedia =temp[['Subject','content', 'filename']]\n","\n","#df_wikipedia.content =df_wikipedia.content.replace(to_replace='<[^<]+?>',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='<p(.*?)>',value='<p>',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='<link(.*?)>',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='-',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='  ',value='',regex=True)\n","df_wikipedia.content =df_wikipedia.content.apply(lambda x:x.strip())\n","\n","# df_wikipedia.to_csv('df_wiki.csv')\n","#print(df_wikipedia.content)\n","\n","## data cleaning for subject data \n","df_wikipedia.Subject =df_wikipedia.Subject.replace(to_replace='[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]',value=' ',regex=True)\n","df_wikipedia.Subject =df_wikipedia.Subject.replace(to_replace='\\s+',value=' ',regex=True)\n","df_wikipedia.Subject =df_wikipedia.Subject.replace(to_replace='  ',value='',regex=True)\n","df_wikipedia.Subject =df_wikipedia.Subject.apply(lambda x:x.strip())\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='<p> </p>',value='',regex=True)\n","\n","df_wikipedia[\"content\"]=df_wikipedia[\"content\"].str.split(\"\\n<p>\")\n","df_wikipedia = df_wikipedia.explode(\"content\").reset_index(drop=True)\n","\n","#print(df_wikipedia[\"content\"].str.split(\"</p>\\s+<h\")[5])\n","df_wikipedia[\"content\"]=df_wikipedia[\"content\"].str.split(\"</p>\\s+<h\")\n","df_wikipedia = df_wikipedia.explode(\"content\").reset_index(drop=True)\n","\n","df_wikipedia[\"content\"]=df_wikipedia[\"content\"].str.split(\"\\n</p>\\n\")\n","df_wikipedia = df_wikipedia.explode(\"content\").reset_index(drop=True)\n","\n","df_wikipedia[\"content\"]=df_wikipedia[\"content\"].str.split(\"</li></ul><h3(.*?)>\")\n","df_wikipedia = df_wikipedia.explode(\"content\").reset_index(drop=True)\n","\n","#print(df_wikipedia[\"content\"][1014])\n","\n","df_wikipedia[\"content\"]=df_wikipedia[\"content\"].str.split(\"</h3>\\s+<ul><li>\")\n","df_wikipedia = df_wikipedia.explode(\"content\").reset_index(drop=True)\n","\n","df_wikipedia['content']=[entry.lower() for entry in df_wikipedia['content']]\n","SW=['<span>', '<li>', '</li>', '<ul>', '</ul>', '<b>', '</b>']\n","for sw in SW:\n","    df_wikipedia.content=df_wikipedia.content.replace(to_replace=sw,value='',regex=True)\n","\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='\\n',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='<\\/p>\\s+<h(.*?)</h(.*?)>',value='',regex=True)  \n","\n","df_wikipedia = df_wikipedia[df_wikipedia.content != ' ']\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='[!\"#$%&\\'()*+,:;=?@[\\\\]^_`{|}~]',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='<[^<]+?>',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='\\s+',value=' ',regex=True)\n","df_wikipedia.content =df_wikipedia.content.replace(to_replace='  ',value=' ',regex=True)\n","\n","df_wikipedia = df_wikipedia[~df_wikipedia['content'].str.startswith('2>')]\n","df_wikipedia = df_wikipedia[~df_wikipedia['content'].str.startswith('3>')]\n","df_wikipedia = df_wikipedia[~df_wikipedia['content'].str.startswith('4>')]\n","df_wikipedia = df_wikipedia[~df_wikipedia['content'].str.startswith('5>')]\n","\n","df_wikipedia = df_wikipedia.reset_index(drop=True)\n","df_wikipedia = df_wikipedia.loc[df_wikipedia['content'].str.len() > 1]\n","df_wikipedia = df_wikipedia.reset_index(drop=True)\n","\n","df_wikipedia[\"tokenized\"] = df_wikipedia[\"content\"]\n","\n","# df_wikipedia = df_wikipedia[df_wikipedia['content'].notna()]\n","print('----------')\n","#print(len(df_wikipedia['content'][28]))\n","df_wikipedia.to_csv('df_wiki.csv')"]},{"cell_type":"markdown","metadata":{"id":"DGuYKjks1h_v"},"source":["### Word Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tcvHtkE1h_v"},"outputs":[],"source":["df_wikipedia['Word tokenize']= [word_tokenize(entry) for entry in df_wikipedia.tokenized]"]},{"cell_type":"markdown","metadata":{"id":"lu2zANRV1h_w"},"source":["### Word Lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LVj_leaw1h_w"},"outputs":[],"source":["# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n","def wordLemmatizer(data):\n","    tag_map = defaultdict(lambda : wn.NOUN)\n","    tag_map['J'] = wn.ADJ\n","    tag_map['V'] = wn.VERB\n","    tag_map['R'] = wn.ADV\n","    file_clean_k =pd.DataFrame()\n","    for index,entry in enumerate(data):\n","        \n","        # Declaring Empty List to store the words that follow the rules for this step\n","        Final_words = []\n","        # Initializing WordNetLemmatizer()\n","        word_Lemmatized = WordNetLemmatizer()\n","        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n","        for word, tag in pos_tag(entry):\n","            # Below condition is to check for Stop words and consider only alphabets\n","            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n","                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n","                Final_words.append(word_Final)\n","            # The final processed set of words for each iteration will be stored in 'text_final'\n","                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n","                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n","                #file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n","                #file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n","                #file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n","                #file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n","    return file_clean_k"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vjw1-czq1h_w","executionInfo":{"status":"ok","timestamp":1663845897193,"user_tz":-120,"elapsed":17,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"3fdc4d1c-71f8-42cb-e47c-97d946ad4456"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(829, 5)"]},"metadata":{},"execution_count":58}],"source":["df_wikipedia.shape "]},{"cell_type":"code","source":["df_wikipedia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"DbicCn4Rmdy9","executionInfo":{"status":"ok","timestamp":1663845897196,"user_tz":-120,"elapsed":17,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"93d5a708-ee7d-4b18-d50f-eb11e26b8052"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                       Subject  \\\n","0    Coronacrisis in Nederland   \n","1    Coronacrisis in Nederland   \n","2    Coronacrisis in Nederland   \n","3    Coronacrisis in Nederland   \n","4    Coronacrisis in Nederland   \n","..                         ...   \n","824                    CureVac   \n","825                    CureVac   \n","826                    CureVac   \n","827                    CureVac   \n","828                    CureVac   \n","\n","                                               content  \\\n","0     de coronacrisis in nederland is onderdeel van...   \n","1    begin 2020 werd sars cov 2 – het coronavirus d...   \n","2    het maatschappelijke verkeer werd ontwricht do...   \n","3    begin 2021 werd een grootschalig vaccinatiepro...   \n","4    het meest relevante cijfer om het verloop van ...   \n","..                                                 ...   \n","824  in 2007 curevac received the innovation prize ...   \n","825  by 2017 curevac had received approximately €30...   \n","826  on 11 march 2020 it was reported that curevac ...   \n","827  cvncov is an mrna vaccine that encodes a minim...   \n","828     media related to curevac at wikimedia commons    \n","\n","                      filename  \\\n","0    Coronacrisis_in_Nederland   \n","1    Coronacrisis_in_Nederland   \n","2    Coronacrisis_in_Nederland   \n","3    Coronacrisis_in_Nederland   \n","4    Coronacrisis_in_Nederland   \n","..                         ...   \n","824                    CureVac   \n","825                    CureVac   \n","826                    CureVac   \n","827                    CureVac   \n","828                    CureVac   \n","\n","                                             tokenized  \\\n","0     de coronacrisis in nederland is onderdeel van...   \n","1    begin 2020 werd sars cov 2 – het coronavirus d...   \n","2    het maatschappelijke verkeer werd ontwricht do...   \n","3    begin 2021 werd een grootschalig vaccinatiepro...   \n","4    het meest relevante cijfer om het verloop van ...   \n","..                                                 ...   \n","824  in 2007 curevac received the innovation prize ...   \n","825  by 2017 curevac had received approximately €30...   \n","826  on 11 march 2020 it was reported that curevac ...   \n","827  cvncov is an mrna vaccine that encodes a minim...   \n","828     media related to curevac at wikimedia commons    \n","\n","                                         Word tokenize  \n","0    [de, coronacrisis, in, nederland, is, onderdee...  \n","1    [begin, 2020, werd, sars, cov, 2, –, het, coro...  \n","2    [het, maatschappelijke, verkeer, werd, ontwric...  \n","3    [begin, 2021, werd, een, grootschalig, vaccina...  \n","4    [het, meest, relevante, cijfer, om, het, verlo...  \n","..                                                 ...  \n","824  [in, 2007, curevac, received, the, innovation,...  \n","825  [by, 2017, curevac, had, received, approximate...  \n","826  [on, 11, march, 2020, it, was, reported, that,...  \n","827  [cvncov, is, an, mrna, vaccine, that, encodes,...  \n","828  [media, related, to, curevac, at, wikimedia, c...  \n","\n","[829 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-fc6b48a5-9221-4f91-9fe7-ea7e623ae9a0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>content</th>\n","      <th>filename</th>\n","      <th>tokenized</th>\n","      <th>Word tokenize</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>de coronacrisis in nederland is onderdeel van...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>de coronacrisis in nederland is onderdeel van...</td>\n","      <td>[de, coronacrisis, in, nederland, is, onderdee...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>begin 2020 werd sars cov 2 – het coronavirus d...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>begin 2020 werd sars cov 2 – het coronavirus d...</td>\n","      <td>[begin, 2020, werd, sars, cov, 2, –, het, coro...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>het maatschappelijke verkeer werd ontwricht do...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>het maatschappelijke verkeer werd ontwricht do...</td>\n","      <td>[het, maatschappelijke, verkeer, werd, ontwric...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>begin 2021 werd een grootschalig vaccinatiepro...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>begin 2021 werd een grootschalig vaccinatiepro...</td>\n","      <td>[begin, 2021, werd, een, grootschalig, vaccina...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>het meest relevante cijfer om het verloop van ...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>het meest relevante cijfer om het verloop van ...</td>\n","      <td>[het, meest, relevante, cijfer, om, het, verlo...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>824</th>\n","      <td>CureVac</td>\n","      <td>in 2007 curevac received the innovation prize ...</td>\n","      <td>CureVac</td>\n","      <td>in 2007 curevac received the innovation prize ...</td>\n","      <td>[in, 2007, curevac, received, the, innovation,...</td>\n","    </tr>\n","    <tr>\n","      <th>825</th>\n","      <td>CureVac</td>\n","      <td>by 2017 curevac had received approximately €30...</td>\n","      <td>CureVac</td>\n","      <td>by 2017 curevac had received approximately €30...</td>\n","      <td>[by, 2017, curevac, had, received, approximate...</td>\n","    </tr>\n","    <tr>\n","      <th>826</th>\n","      <td>CureVac</td>\n","      <td>on 11 march 2020 it was reported that curevac ...</td>\n","      <td>CureVac</td>\n","      <td>on 11 march 2020 it was reported that curevac ...</td>\n","      <td>[on, 11, march, 2020, it, was, reported, that,...</td>\n","    </tr>\n","    <tr>\n","      <th>827</th>\n","      <td>CureVac</td>\n","      <td>cvncov is an mrna vaccine that encodes a minim...</td>\n","      <td>CureVac</td>\n","      <td>cvncov is an mrna vaccine that encodes a minim...</td>\n","      <td>[cvncov, is, an, mrna, vaccine, that, encodes,...</td>\n","    </tr>\n","    <tr>\n","      <th>828</th>\n","      <td>CureVac</td>\n","      <td>media related to curevac at wikimedia commons</td>\n","      <td>CureVac</td>\n","      <td>media related to curevac at wikimedia commons</td>\n","      <td>[media, related, to, curevac, at, wikimedia, c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>829 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc6b48a5-9221-4f91-9fe7-ea7e623ae9a0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fc6b48a5-9221-4f91-9fe7-ea7e623ae9a0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fc6b48a5-9221-4f91-9fe7-ea7e623ae9a0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":59}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":632},"id":"HjPPlzKy1h_x","executionInfo":{"status":"ok","timestamp":1663845976920,"user_tz":-120,"elapsed":79738,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"65df9077-86af-48c1-ff8c-abd573b21b04"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["                                         Keyword_final\n","0    ['de', 'coronacrisis', 'nederland', 'onderdeel...\n","1    ['begin', 'werd', 'sars', 'cov', 'het', 'coron...\n","2    ['het', 'maatschappelijke', 'verkeer', 'werd',...\n","3    ['begin', 'werd', 'een', 'grootschalig', 'vacc...\n","4    ['het', 'meest', 'relevante', 'cijfer', 'om', ...\n","..                                                 ...\n","824  ['curevac', 'receive', 'innovation', 'prize', ...\n","825  ['curevac', 'receive', 'approximately', 'milli...\n","826  ['march', 'report', 'curevac', 'ag', 'ceo', 'd...\n","827  ['cvncov', 'mrna', 'vaccine', 'encode', 'minim...\n","828  ['medium', 'relate', 'curevac', 'wikimedia', '...\n","\n","[829 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-631224a6-30fd-43af-8c0f-dd7a4fffe388\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Keyword_final</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>['de', 'coronacrisis', 'nederland', 'onderdeel...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>['begin', 'werd', 'sars', 'cov', 'het', 'coron...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>['het', 'maatschappelijke', 'verkeer', 'werd',...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>['begin', 'werd', 'een', 'grootschalig', 'vacc...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>['het', 'meest', 'relevante', 'cijfer', 'om', ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>824</th>\n","      <td>['curevac', 'receive', 'innovation', 'prize', ...</td>\n","    </tr>\n","    <tr>\n","      <th>825</th>\n","      <td>['curevac', 'receive', 'approximately', 'milli...</td>\n","    </tr>\n","    <tr>\n","      <th>826</th>\n","      <td>['march', 'report', 'curevac', 'ag', 'ceo', 'd...</td>\n","    </tr>\n","    <tr>\n","      <th>827</th>\n","      <td>['cvncov', 'mrna', 'vaccine', 'encode', 'minim...</td>\n","    </tr>\n","    <tr>\n","      <th>828</th>\n","      <td>['medium', 'relate', 'curevac', 'wikimedia', '...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>829 rows × 1 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-631224a6-30fd-43af-8c0f-dd7a4fffe388')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-631224a6-30fd-43af-8c0f-dd7a4fffe388 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-631224a6-30fd-43af-8c0f-dd7a4fffe388');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":60}],"source":["import pandas as pd\n","import numpy as np\n","import os \n","import re\n","import operator\n","import pickle\n","import nltk \n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from collections import defaultdict\n","from nltk.corpus import wordnet as wn\n","import os.path\n","import json\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","df_clean = wordLemmatizer(df_wikipedia['Word tokenize'][0:989]) \n","df_clean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8O6bDkF1h_x"},"outputs":[],"source":["df_clean=df_clean.replace(to_replace =\"\\[.\", value = '', regex = True)\n","df_clean=df_clean.replace(to_replace =\"'\", value = '', regex = True)\n","df_clean=df_clean.replace(to_replace =\" \", value = '', regex = True)\n","df_clean=df_clean.replace(to_replace ='\\]', value = '', regex = True)"]},{"cell_type":"markdown","metadata":{"id":"cqrqQPe71h_y"},"source":["### Added WordLemmatize words into given dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"OWqTOXJl1h_y","executionInfo":{"status":"ok","timestamp":1663845976927,"user_tz":-120,"elapsed":25,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"54d5913e-298c-4583-aa77-9f4ba399f230"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                       Subject  \\\n","0    Coronacrisis in Nederland   \n","1    Coronacrisis in Nederland   \n","2    Coronacrisis in Nederland   \n","3    Coronacrisis in Nederland   \n","4    Coronacrisis in Nederland   \n","..                         ...   \n","824                    CureVac   \n","825                    CureVac   \n","826                    CureVac   \n","827                    CureVac   \n","828                    CureVac   \n","\n","                                               content  \\\n","0     de coronacrisis in nederland is onderdeel van...   \n","1    begin 2020 werd sars cov 2 – het coronavirus d...   \n","2    het maatschappelijke verkeer werd ontwricht do...   \n","3    begin 2021 werd een grootschalig vaccinatiepro...   \n","4    het meest relevante cijfer om het verloop van ...   \n","..                                                 ...   \n","824  in 2007 curevac received the innovation prize ...   \n","825  by 2017 curevac had received approximately €30...   \n","826  on 11 march 2020 it was reported that curevac ...   \n","827  cvncov is an mrna vaccine that encodes a minim...   \n","828     media related to curevac at wikimedia commons    \n","\n","                      filename  \\\n","0    Coronacrisis_in_Nederland   \n","1    Coronacrisis_in_Nederland   \n","2    Coronacrisis_in_Nederland   \n","3    Coronacrisis_in_Nederland   \n","4    Coronacrisis_in_Nederland   \n","..                         ...   \n","824                    CureVac   \n","825                    CureVac   \n","826                    CureVac   \n","827                    CureVac   \n","828                    CureVac   \n","\n","                                         Clean_Keyword  \\\n","0    de,coronacrisis,nederland,onderdeel,van,de,wer...   \n","1    begin,werd,sars,cov,het,coronavirus,dat,covid,...   \n","2    het,maatschappelijke,verkeer,werd,ontwricht,do...   \n","3    begin,werd,een,grootschalig,vaccinatieprogramm...   \n","4    het,meest,relevante,cijfer,om,het,verloop,van,...   \n","..                                                 ...   \n","824  curevac,receive,innovation,prize,state,baden,w...   \n","825  curevac,receive,approximately,million,equity,i...   \n","826  march,report,curevac,ag,ceo,daniel,menichella,...   \n","827  cvncov,mrna,vaccine,encode,minimal,piece,coron...   \n","828             medium,relate,curevac,wikimedia,common   \n","\n","                                             tokenized  \\\n","0     de coronacrisis in nederland is onderdeel van...   \n","1    begin 2020 werd sars cov 2 – het coronavirus d...   \n","2    het maatschappelijke verkeer werd ontwricht do...   \n","3    begin 2021 werd een grootschalig vaccinatiepro...   \n","4    het meest relevante cijfer om het verloop van ...   \n","..                                                 ...   \n","824  in 2007 curevac received the innovation prize ...   \n","825  by 2017 curevac had received approximately €30...   \n","826  on 11 march 2020 it was reported that curevac ...   \n","827  cvncov is an mrna vaccine that encodes a minim...   \n","828     media related to curevac at wikimedia commons    \n","\n","                                         Word tokenize  \n","0    [de, coronacrisis, in, nederland, is, onderdee...  \n","1    [begin, 2020, werd, sars, cov, 2, –, het, coro...  \n","2    [het, maatschappelijke, verkeer, werd, ontwric...  \n","3    [begin, 2021, werd, een, grootschalig, vaccina...  \n","4    [het, meest, relevante, cijfer, om, het, verlo...  \n","..                                                 ...  \n","824  [in, 2007, curevac, received, the, innovation,...  \n","825  [by, 2017, curevac, had, received, approximate...  \n","826  [on, 11, march, 2020, it, was, reported, that,...  \n","827  [cvncov, is, an, mrna, vaccine, that, encodes,...  \n","828  [media, related, to, curevac, at, wikimedia, c...  \n","\n","[829 rows x 6 columns]"],"text/html":["\n","  <div id=\"df-83e486b8-dc39-4257-a64b-9229e11f04f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Subject</th>\n","      <th>content</th>\n","      <th>filename</th>\n","      <th>Clean_Keyword</th>\n","      <th>tokenized</th>\n","      <th>Word tokenize</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>de coronacrisis in nederland is onderdeel van...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>de,coronacrisis,nederland,onderdeel,van,de,wer...</td>\n","      <td>de coronacrisis in nederland is onderdeel van...</td>\n","      <td>[de, coronacrisis, in, nederland, is, onderdee...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>begin 2020 werd sars cov 2 – het coronavirus d...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>begin,werd,sars,cov,het,coronavirus,dat,covid,...</td>\n","      <td>begin 2020 werd sars cov 2 – het coronavirus d...</td>\n","      <td>[begin, 2020, werd, sars, cov, 2, –, het, coro...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>het maatschappelijke verkeer werd ontwricht do...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>het,maatschappelijke,verkeer,werd,ontwricht,do...</td>\n","      <td>het maatschappelijke verkeer werd ontwricht do...</td>\n","      <td>[het, maatschappelijke, verkeer, werd, ontwric...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>begin 2021 werd een grootschalig vaccinatiepro...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>begin,werd,een,grootschalig,vaccinatieprogramm...</td>\n","      <td>begin 2021 werd een grootschalig vaccinatiepro...</td>\n","      <td>[begin, 2021, werd, een, grootschalig, vaccina...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Coronacrisis in Nederland</td>\n","      <td>het meest relevante cijfer om het verloop van ...</td>\n","      <td>Coronacrisis_in_Nederland</td>\n","      <td>het,meest,relevante,cijfer,om,het,verloop,van,...</td>\n","      <td>het meest relevante cijfer om het verloop van ...</td>\n","      <td>[het, meest, relevante, cijfer, om, het, verlo...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>824</th>\n","      <td>CureVac</td>\n","      <td>in 2007 curevac received the innovation prize ...</td>\n","      <td>CureVac</td>\n","      <td>curevac,receive,innovation,prize,state,baden,w...</td>\n","      <td>in 2007 curevac received the innovation prize ...</td>\n","      <td>[in, 2007, curevac, received, the, innovation,...</td>\n","    </tr>\n","    <tr>\n","      <th>825</th>\n","      <td>CureVac</td>\n","      <td>by 2017 curevac had received approximately €30...</td>\n","      <td>CureVac</td>\n","      <td>curevac,receive,approximately,million,equity,i...</td>\n","      <td>by 2017 curevac had received approximately €30...</td>\n","      <td>[by, 2017, curevac, had, received, approximate...</td>\n","    </tr>\n","    <tr>\n","      <th>826</th>\n","      <td>CureVac</td>\n","      <td>on 11 march 2020 it was reported that curevac ...</td>\n","      <td>CureVac</td>\n","      <td>march,report,curevac,ag,ceo,daniel,menichella,...</td>\n","      <td>on 11 march 2020 it was reported that curevac ...</td>\n","      <td>[on, 11, march, 2020, it, was, reported, that,...</td>\n","    </tr>\n","    <tr>\n","      <th>827</th>\n","      <td>CureVac</td>\n","      <td>cvncov is an mrna vaccine that encodes a minim...</td>\n","      <td>CureVac</td>\n","      <td>cvncov,mrna,vaccine,encode,minimal,piece,coron...</td>\n","      <td>cvncov is an mrna vaccine that encodes a minim...</td>\n","      <td>[cvncov, is, an, mrna, vaccine, that, encodes,...</td>\n","    </tr>\n","    <tr>\n","      <th>828</th>\n","      <td>CureVac</td>\n","      <td>media related to curevac at wikimedia commons</td>\n","      <td>CureVac</td>\n","      <td>medium,relate,curevac,wikimedia,common</td>\n","      <td>media related to curevac at wikimedia commons</td>\n","      <td>[media, related, to, curevac, at, wikimedia, c...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>829 rows × 6 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83e486b8-dc39-4257-a64b-9229e11f04f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-83e486b8-dc39-4257-a64b-9229e11f04f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-83e486b8-dc39-4257-a64b-9229e11f04f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":62}],"source":["# df_wikipedia_save= df_clean\n","# df_wikipedia_save\n","df_wikipedia.insert(loc=3, column='Clean_Keyword', value=df_clean['Keyword_final'].tolist())\n","df_wikipedia_save= df_wikipedia\n","#df_wikipedia_save = df_wikipedia_save.drop(['Word tokenize,'Clean_Keyword'],axis=1)\n","#df_wikipedia = df_wikipedia.drop(['Word tokenize','Clean_Keyword'],axis=1)\n","df_wikipedia_save"]},{"cell_type":"markdown","source":["## TF-IDF"],"metadata":{"id":"jyy0UlH4o_tX"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import operator\n","\n","## Create Vocabulary\n","vocabulary = set()\n","\n","for doc in df_wikipedia.Clean_Keyword:\n","    vocabulary.update(doc.split(','))\n","\n","vocabulary = list(vocabulary)\n","\n","# Intializating the tfIdf model\n","tfidf = TfidfVectorizer(vocabulary=vocabulary,dtype=np.float32)\n","\n","print(tfidf)\n","\n","# Fit the TfIdf model\n","tfidf.fit(df_wikipedia.Clean_Keyword)\n","\n","# Transform the TfIdf model\n","tfidf_tran=tfidf.transform(df_wikipedia.Clean_Keyword)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-s2Pi-A2pC3w","executionInfo":{"status":"ok","timestamp":1663845977288,"user_tz":-120,"elapsed":382,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"961d908f-69ea-40bb-d860-1acc9d212168"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TfidfVectorizer(dtype=<class 'numpy.float32'>,\n","                vocabulary=['betrouwbare', 'contraception', 'dance',\n","                            'identiteitsbewijs', 'available', 'harmony',\n","                            'verrichten', 'msf', 'bliezen', 'tenacious',\n","                            'konden', 'eastern', 'jarenlange', 'hashtag',\n","                            'waldeyer', 'national', 'equality', 'spoke',\n","                            'similarity', 'veelvuldig', 'zuofeng', 'hardly',\n","                            'aidsstadium', 'minimaliseren', 'consortium',\n","                            'kritische', 'ziek', 'property', 'papr', 'huis', ...])\n"]}]},{"cell_type":"code","source":["### Save model\n","with open('tfid.pkl','wb') as handle:\n","    pickle.dump(tfidf_tran, handle)\n","\n","### load model\n","t = pickle.load(open('tfid.pkl','rb'))\n","\n","### Save Vacabulary\n","with open(\"vocab_wiki_files.txt\", \"w\") as file:\n","    file.write(str(vocabulary))\n","\n","### load Vacabulary\n","with open(\"vocab_wiki_files.txt\", \"r\") as file:\n","    data2 = eval(file.readline())"],"metadata":{"id":"AaUvIwSRtYTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_vector_T(tokens):\n","    Q = np.zeros((len(vocabulary)))\n","    x= tfidf.transform(tokens)\n","    for token in tokens[0].split(','):\n","        try:\n","            ind = vocabulary.index(token)\n","            Q[ind]  = x[0, tfidf.vocabulary_[token]]\n","        except:\n","            pass\n","    return Q"],"metadata":{"id":"nh6wsD9GrvRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cosine_sim(a, b):\n","    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n","    return cos_sim"],"metadata":{"id":"Lld-ERIbrw9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cosine_similarity_T(k, query):\n","    #print(\"Cosine Similarity\")\n","    preprocessed_query = re.sub(\"\\W+\", \" \", query).strip()\n","    tokens = word_tokenize(str(preprocessed_query))\n","    q_df = pd.DataFrame(columns=['q_clean'])\n","    q_df.loc[0,'q_clean'] =tokens\n","    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)\n","    q_df=q_df.replace(to_replace =\"\\[.\", value = '', regex = True)\n","    q_df=q_df.replace(to_replace =\"'\", value = '', regex = True)\n","    q_df=q_df.replace(to_replace =\" \", value = '', regex = True)\n","    q_df=q_df.replace(to_replace ='\\]', value = '', regex = True)\n","\n","    d_cosines = []\n","\n","    query_vector = gen_vector_T(q_df['q_clean'])\n","    \n","    print(query_vector)\n","    for d in tfidf_tran.A:\n","        \n","        d_cosines.append(cosine_sim(query_vector, d))\n","                    \n","    out = np.array(d_cosines).argsort()[-k:][::-1]\n","    #print(\"\")\n","    d_cosines.sort()\n","    #print(out)\n","    a = pd.DataFrame()\n","    for i,index in enumerate(out):\n","        a.loc[i,'index'] = str(index)\n","        a.loc[i,'Subject'] = df_wikipedia['Subject'][index]\n","        a.loc[i,'content'] = df_wikipedia['content'][index]\n","    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n","        a.loc[j,'Score'] = simScore\n","    return a"],"metadata":{"id":"mA9g-Xs_ryom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%time cosine_similarity_T(10,'johnson and johnson vaccines derived from murdered babies. at least 2 other vaccines are not. if you take vaccines use those.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"AEjh_o4hr2fc","executionInfo":{"status":"ok","timestamp":1663845977521,"user_tz":-120,"elapsed":242,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"1b048dea-9595-4407-8d5c-3f0f0dbaacfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. ... 0. 0. 0.]\n","CPU times: user 132 ms, sys: 87.2 ms, total: 219 ms\n","Wall time: 139 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["  index                                  Subject  \\\n","0   750                         COVID 19 vaccine   \n","1   743                         COVID 19 vaccine   \n","2   734                         COVID 19 vaccine   \n","3   742                         COVID 19 vaccine   \n","4   766                         COVID 19 vaccine   \n","5   339  Chinese government response to COVID 19   \n","6   735                         COVID 19 vaccine   \n","7   765                         COVID 19 vaccine   \n","8   741                         COVID 19 vaccine   \n","9   749                         COVID 19 vaccine   \n","\n","                                             content     Score  \n","0  internationally the access to covid‑19 tools a...  0.340187  \n","1  additional types of vaccines that are in clini...  0.339199  \n","2  a covid‑19 vaccine is a vaccine intended to pr...  0.333803  \n","3  subunit vaccines present one or more antigens ...  0.328737  \n","4   protocol mrna 1273 p301 pdf . moderna. protoc...  0.319484  \n","5  in july 2020 the chinese government granted an...  0.317339  \n","6  prior to covid‑19 a vaccine for an infectious ...  0.293005  \n","7                                 vaccine protocols   0.286098  \n","8  inactivated vaccines consist of virus particle...  0.271578  \n","9  there have been several unique challenges with...  0.271332  "],"text/html":["\n","  <div id=\"df-16c63e5d-2a04-4309-9f65-0bb3579b0cea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Subject</th>\n","      <th>content</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>750</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>internationally the access to covid‑19 tools a...</td>\n","      <td>0.340187</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>743</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>additional types of vaccines that are in clini...</td>\n","      <td>0.339199</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>734</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>a covid‑19 vaccine is a vaccine intended to pr...</td>\n","      <td>0.333803</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>742</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>subunit vaccines present one or more antigens ...</td>\n","      <td>0.328737</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>766</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>protocol mrna 1273 p301 pdf . moderna. protoc...</td>\n","      <td>0.319484</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>339</td>\n","      <td>Chinese government response to COVID 19</td>\n","      <td>in july 2020 the chinese government granted an...</td>\n","      <td>0.317339</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>735</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>prior to covid‑19 a vaccine for an infectious ...</td>\n","      <td>0.293005</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>765</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>vaccine protocols</td>\n","      <td>0.286098</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>741</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>inactivated vaccines consist of virus particle...</td>\n","      <td>0.271578</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>749</td>\n","      <td>COVID 19 vaccine</td>\n","      <td>there have been several unique challenges with...</td>\n","      <td>0.271332</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16c63e5d-2a04-4309-9f65-0bb3579b0cea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-16c63e5d-2a04-4309-9f65-0bb3579b0cea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-16c63e5d-2a04-4309-9f65-0bb3579b0cea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["## Google Sentence Encoder"],"metadata":{"id":"PqAFfTijWMV6"}},{"cell_type":"code","source":["!pip install --upgrade tensorflow-gpu\n","#Install TF-Hub.\n","!pip install tensorflow-hub\n","!pip install seaborn\n","\n","!pip3 install tensorflow-text --force-reinstall\n","import tensorflow_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cPsjieaimvpF","executionInfo":{"status":"error","timestamp":1662973548123,"user_tz":-120,"elapsed":82165,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"19393606-81f3-47f1-c705-8d7969dbf478"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.10.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.19.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.0.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (14.0.6)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.27.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.0.7)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (65.3.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.48.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n","Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.2.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.14.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.6)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.16.0)\n","Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (21.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (4.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (3.4.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.11.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow-gpu) (2.28.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow-gpu) (0.4.8)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (1.26.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2022.6.15.1)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (3.2.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-gpu) (2.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.6)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.19.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.7.3)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.2.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.16.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text\n","  Using cached tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","Collecting tensorflow-hub>=0.8.0\n","  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n","Collecting tensorflow<2.11,>=2.10.0\n","  Using cached tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n","Collecting h5py>=2.9.0\n","  Using cached h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n","Collecting libclang>=13.0.0\n","  Using cached libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n","Collecting google-pasta>=0.1.1\n","  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Using cached tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n","Collecting termcolor>=1.1.0\n","  Using cached termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n","Collecting absl-py>=1.0.0\n","  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n","Collecting protobuf<3.20,>=3.9.2\n","  Using cached protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","Collecting numpy>=1.20\n","  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","Collecting setuptools\n","  Using cached setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n","Collecting grpcio<2.0,>=1.24.3\n","  Using cached grpcio-1.48.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","Collecting tensorboard<2.11,>=2.10\n","  Using cached tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n","Collecting packaging\n","  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n","Collecting typing-extensions>=3.6.6\n","  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n","Collecting tensorflow-estimator<2.11,>=2.10.0\n","  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","Collecting six>=1.12.0\n","  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting gast<=0.4.0,>=0.2.1\n","  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting astunparse>=1.6.0\n","  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Collecting keras-preprocessing>=1.1.1\n","  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Collecting keras<2.11,>=2.10.0\n","  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n","Collecting opt-einsum>=2.3.2\n","  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","Collecting wrapt>=1.11.0\n","  Using cached wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","Collecting flatbuffers>=2.0\n","  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n","Collecting wheel<1.0,>=0.23.0\n","  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","Collecting requests<3,>=2.21.0\n","  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n","Collecting werkzeug>=1.0.1\n","  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0\n","  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","Collecting markdown>=2.6.8\n","  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n","Collecting google-auth<3,>=1.6.3\n","  Using cached google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1\n","  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting rsa<5,>=3.1.4\n","  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n","Collecting cachetools<6.0,>=2.0.0\n","  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n","Collecting pyasn1-modules>=0.2.1\n","  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n","Collecting requests-oauthlib>=0.7.0\n","  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Collecting importlib-metadata>=4.4\n","  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n","Collecting zipp>=0.5\n","  Using cached zipp-3.8.1-py3-none-any.whl (5.6 kB)\n","Collecting pyasn1<0.5.0,>=0.4.6\n","  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n","Collecting charset-normalizer<3,>=2\n","  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n","Collecting idna<4,>=2.5\n","  Using cached idna-3.3-py3-none-any.whl (61 kB)\n","Collecting certifi>=2017.4.17\n","  Using cached certifi-2022.6.15.1-py3-none-any.whl (160 kB)\n","Collecting urllib3<1.27,>=1.21.1\n","  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","Collecting oauthlib>=3.0.0\n","  Using cached oauthlib-3.2.1-py3-none-any.whl (151 kB)\n","Collecting MarkupSafe>=2.1.1\n","  Using cached MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Collecting pyparsing!=3.0.5,>=2.0.2\n","  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-hub, tensorflow, tensorflow-text\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.26.12\n","    Uninstalling urllib3-1.26.12:\n","      Successfully uninstalled urllib3-1.26.12\n","  Attempting uninstall: pyasn1\n","    Found existing installation: pyasn1 0.4.8\n","    Uninstalling pyasn1-0.4.8:\n","      Successfully uninstalled pyasn1-0.4.8\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.3\n","    Uninstalling idna-3.3:\n","      Successfully uninstalled idna-3.3\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 2.1.1\n","    Uninstalling charset-normalizer-2.1.1:\n","      Successfully uninstalled charset-normalizer-2.1.1\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2022.6.15.1\n","    Uninstalling certifi-2022.6.15.1:\n","      Successfully uninstalled certifi-2022.6.15.1\n","  Attempting uninstall: zipp\n","    Found existing installation: zipp 3.8.1\n","    Uninstalling zipp-3.8.1:\n","      Successfully uninstalled zipp-3.8.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.3.0\n","    Uninstalling typing-extensions-4.3.0:\n","      Successfully uninstalled typing-extensions-4.3.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: rsa\n","    Found existing installation: rsa 4.9\n","    Uninstalling rsa-4.9:\n","      Successfully uninstalled rsa-4.9\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.28.1\n","    Uninstalling requests-2.28.1:\n","      Successfully uninstalled requests-2.28.1\n","  Attempting uninstall: pyasn1-modules\n","    Found existing installation: pyasn1-modules 0.2.8\n","    Uninstalling pyasn1-modules-0.2.8:\n","      Successfully uninstalled pyasn1-modules-0.2.8\n","  Attempting uninstall: oauthlib\n","    Found existing installation: oauthlib 3.2.1\n","    Uninstalling oauthlib-3.2.1:\n","      Successfully uninstalled oauthlib-3.2.1\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.2.0\n","    Uninstalling cachetools-5.2.0:\n","      Successfully uninstalled cachetools-5.2.0\n","  Attempting uninstall: requests-oauthlib\n","    Found existing installation: requests-oauthlib 1.3.1\n","    Uninstalling requests-oauthlib-1.3.1:\n","      Successfully uninstalled requests-oauthlib-1.3.1\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.1\n","    Uninstalling MarkupSafe-2.1.1:\n","      Successfully uninstalled MarkupSafe-2.1.1\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.12.0\n","    Uninstalling importlib-metadata-4.12.0:\n","      Successfully uninstalled importlib-metadata-4.12.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 2.11.0\n","    Uninstalling google-auth-2.11.0:\n","      Successfully uninstalled google-auth-2.11.0\n","  Attempting uninstall: wheel\n","    Found existing installation: wheel 0.37.1\n","    Uninstalling wheel-0.37.1:\n","      Successfully uninstalled wheel-0.37.1\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 2.2.2\n","    Uninstalling Werkzeug-2.2.2:\n","      Successfully uninstalled Werkzeug-2.2.2\n","  Attempting uninstall: tensorboard-plugin-wit\n","    Found existing installation: tensorboard-plugin-wit 1.8.1\n","    Uninstalling tensorboard-plugin-wit-1.8.1:\n","      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.6.1\n","    Uninstalling tensorboard-data-server-0.6.1:\n","      Successfully uninstalled tensorboard-data-server-0.6.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 65.3.0\n","    Uninstalling setuptools-65.3.0:\n","      Successfully uninstalled setuptools-65.3.0\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.9\n","    Uninstalling pyparsing-3.0.9:\n","      Successfully uninstalled pyparsing-3.0.9\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.4\n","    Uninstalling protobuf-3.19.4:\n","      Successfully uninstalled protobuf-3.19.4\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: markdown\n","    Found existing installation: Markdown 3.4.1\n","    Uninstalling Markdown-3.4.1:\n","      Successfully uninstalled Markdown-3.4.1\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.48.1\n","    Uninstalling grpcio-1.48.1:\n","      Successfully uninstalled grpcio-1.48.1\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.2.0\n","    Uninstalling absl-py-1.2.0:\n","      Successfully uninstalled absl-py-1.2.0\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.0.1\n","    Uninstalling termcolor-2.0.1:\n","      Successfully uninstalled termcolor-2.0.1\n","  Attempting uninstall: tensorflow-io-gcs-filesystem\n","    Found existing installation: tensorflow-io-gcs-filesystem 0.27.0\n","    Uninstalling tensorflow-io-gcs-filesystem-0.27.0:\n","      Successfully uninstalled tensorflow-io-gcs-filesystem-0.27.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.10.0\n","    Uninstalling tensorflow-estimator-2.10.0:\n","      Successfully uninstalled tensorflow-estimator-2.10.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.10.0\n","    Uninstalling tensorboard-2.10.0:\n","      Successfully uninstalled tensorboard-2.10.0\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 21.3\n","    Uninstalling packaging-21.3:\n","      Successfully uninstalled packaging-21.3\n","  Attempting uninstall: opt-einsum\n","    Found existing installation: opt-einsum 3.3.0\n","    Uninstalling opt-einsum-3.3.0:\n","      Successfully uninstalled opt-einsum-3.3.0\n","  Attempting uninstall: libclang\n","    Found existing installation: libclang 14.0.6\n","    Uninstalling libclang-14.0.6:\n","      Successfully uninstalled libclang-14.0.6\n","  Attempting uninstall: keras-preprocessing\n","    Found existing installation: Keras-Preprocessing 1.1.2\n","    Uninstalling Keras-Preprocessing-1.1.2:\n","      Successfully uninstalled Keras-Preprocessing-1.1.2\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.10.0\n","    Uninstalling keras-2.10.0:\n","      Successfully uninstalled keras-2.10.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.7.0\n","    Uninstalling h5py-3.7.0:\n","      Successfully uninstalled h5py-3.7.0\n","  Attempting uninstall: google-pasta\n","    Found existing installation: google-pasta 0.2.0\n","    Uninstalling google-pasta-0.2.0:\n","      Successfully uninstalled google-pasta-0.2.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0.7\n","    Uninstalling flatbuffers-2.0.7:\n","      Successfully uninstalled flatbuffers-2.0.7\n","  Attempting uninstall: astunparse\n","    Found existing installation: astunparse 1.6.3\n","    Uninstalling astunparse-1.6.3:\n","      Successfully uninstalled astunparse-1.6.3\n","  Attempting uninstall: tensorflow-hub\n","    Found existing installation: tensorflow-hub 0.12.0\n","    Uninstalling tensorflow-hub-0.12.0:\n","      Successfully uninstalled tensorflow-hub-0.12.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.10.0\n","    Uninstalling tensorflow-2.10.0:\n","      Successfully uninstalled tensorflow-2.10.0\n","  Attempting uninstall: tensorflow-text\n","    Found existing installation: tensorflow-text 2.10.0\n","    Uninstalling tensorflow-text-2.10.0:\n","      Successfully uninstalled tensorflow-text-2.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","thinc 8.1.0 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n","google-api-core 1.31.6 requires google-auth<2.0dev,>=1.25.0, but you have google-auth 2.11.0 which is incompatible.\n","flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.2.2 which is incompatible.\u001b[0m\n","Successfully installed MarkupSafe-2.1.1 absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.6.15.1 charset-normalizer-2.1.1 flatbuffers-2.0.7 gast-0.4.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.1 h5py-3.7.0 idna-3.3 importlib-metadata-4.12.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.21.6 oauthlib-3.2.1 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 setuptools-65.3.0 six-1.16.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-text-2.10.0 termcolor-2.0.1 typing-extensions-4.3.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["absl","astunparse","certifi","flatbuffers","gast","google","h5py","idna","importlib_metadata","keras","keras_preprocessing","markupsafe","numpy","opt_einsum","packaging","pkg_resources","pyasn1","pyasn1_modules","requests","six","tensorboard","tensorflow","urllib3","wrapt","zipp"]}}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ecd5385ced20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install tensorflow-text --force-reinstall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpybinds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtflite_registrar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.7/dist-packages/tensorflow_text/core/pybinds/tflite_registrar.so: undefined symbol: _ZN4absl12lts_2022062320raw_logging_internal21internal_log_functionB5cxx11E","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re, string\n","import os \n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics.pairwise import linear_kernel\n","import tensorflow_text\n","\n","\n","module_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n","model = hub.load(module_url)\n","\n","# # #Create function for using modeltraining\n","def embed(input):\n","    return model(input)\n","\n","\n","Model_USE= embed(df_wikipedia['tokenized'])\n","\n","exported = tf.train.Checkpoint(v=tf.Variable(Model_USE))\n","exported.f = tf.function(\n","    lambda  x: exported.v * x,\n","    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n","\n","tf.saved_model.save(exported,'TrainModel')\n","\n","imported = tf.saved_model.load('TrainModel')\n","loadedmodel =imported.v.numpy()\n","loadedmodel.shape\n","\n","def SearchDocument(query):\n","    q =[query]\n","    # embed the query for calcluating the similarity\n","    Q_Train =embed(q)\n","\n","    # Calculate the Similarity\n","    linear_similarities = linear_kernel(Q_Train, loadedmodel).flatten() \n","    #Sort top 10 index with similarity score\n","    Top_index_doc = linear_similarities.argsort()[:-11:-1]\n","    # sort by similarity score\n","    linear_similarities.sort()\n","    a = pd.DataFrame()\n","    for i,index in enumerate(Top_index_doc):\n","        a.loc[i,'index'] = str(index)\n","        a.loc[i,'File_Name'] = df_wikipedia['content'][index] ## Read File name with index from File_data DF\n","    for j,simScore in enumerate(linear_similarities[:-11:-1]):\n","        a.loc[j,'Score'] = simScore\n","    return a\n","\n","SearchDocument('Together with collaborators, we founded a spinout company to develop and commercialise a nasal spray vaccine against COVID. The vaccine is based on research by our and @UniEastFinland researchers. #HelsinkiHealth #corona #COVID19 #covidvaccine')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"ilyS0YLFl_Lf","executionInfo":{"status":"ok","timestamp":1663846642542,"user_tz":-120,"elapsed":8022,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"4e614273-8a3e-43dc-94bb-1ffa26a00ca0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  index                                          File_Name     Score\n","0   741  inactivated vaccines consist of virus particle...  0.625157\n","1   746  a universal coronavirus vaccine is effective a...  0.589395\n","2   740  these vaccines are examples of non replicating...  0.584928\n","3   766   protocol mrna 1273 p301 pdf . moderna. protoc...  0.576884\n","4   745  intranasal vaccines target mucosal immunity in...  0.564301\n","5   739  several covid‑19 vaccines including the pfizer...  0.559850\n","6   742  subunit vaccines present one or more antigens ...  0.558546\n","7   734  a covid‑19 vaccine is a vaccine intended to pr...  0.557974\n","8   750  internationally the access to covid‑19 tools a...  0.557162\n","9   735  prior to covid‑19 a vaccine for an infectious ...  0.549555"],"text/html":["\n","  <div id=\"df-cde0293a-01de-4f78-908e-33f92955a0b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>File_Name</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>741</td>\n","      <td>inactivated vaccines consist of virus particle...</td>\n","      <td>0.625157</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>746</td>\n","      <td>a universal coronavirus vaccine is effective a...</td>\n","      <td>0.589395</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>740</td>\n","      <td>these vaccines are examples of non replicating...</td>\n","      <td>0.584928</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>766</td>\n","      <td>protocol mrna 1273 p301 pdf . moderna. protoc...</td>\n","      <td>0.576884</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>745</td>\n","      <td>intranasal vaccines target mucosal immunity in...</td>\n","      <td>0.564301</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>739</td>\n","      <td>several covid‑19 vaccines including the pfizer...</td>\n","      <td>0.559850</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>742</td>\n","      <td>subunit vaccines present one or more antigens ...</td>\n","      <td>0.558546</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>734</td>\n","      <td>a covid‑19 vaccine is a vaccine intended to pr...</td>\n","      <td>0.557974</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>750</td>\n","      <td>internationally the access to covid‑19 tools a...</td>\n","      <td>0.557162</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>735</td>\n","      <td>prior to covid‑19 a vaccine for an infectious ...</td>\n","      <td>0.549555</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cde0293a-01de-4f78-908e-33f92955a0b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cde0293a-01de-4f78-908e-33f92955a0b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cde0293a-01de-4f78-908e-33f92955a0b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","source":["## Sentence transformers"],"metadata":{"id":"kRN06d4uWTNH"}},{"cell_type":"code","source":["!pip install -U sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97s3pLZcXPIu","executionInfo":{"status":"ok","timestamp":1663847693609,"user_tz":-120,"elapsed":4752,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"923414d2-b4ee-40d1-e642-bd0f4d3ba0a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.22.1)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"]}]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import linear_kernel\n","from numpy import dot\n","from numpy.linalg import norm\n","import scipy\n","\n","model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n","\n","def embed(input):\n","    return model(input)\n","\n","model.max_seq_length = 512\n","print(\"Max Sequence Length:\", model.max_seq_length)\n","embeddings = model.encode(df_wikipedia['content'])\n","\n","def SearchDocument(query):\n","    q =[query]\n","    # embed the query for calcluating the similarity\n","    Q_Train = model.encode(q)\n","\n","    # Calculate the Similarity\n","    #linear_similarities = linear_kernel(Q_Train, embeddings).flatten()\n","    linear_similarities = scipy.spatial.distance.cdist(Q_Train, embeddings, \"cosine\")[0]\n","    linear_similarities = 1 - linear_similarities\n","    #Sort top 10 index with similarity score\n","    Top_index_doc = linear_similarities.argsort()[:-11:-1]\n","    # sort by similarity score\n","    linear_similarities.sort()\n","    a = pd.DataFrame()\n","    for i,index in enumerate(Top_index_doc):\n","        a.loc[i,'index'] = str(index)\n","        a.loc[i,'Content'] = df_wikipedia['content'][index] ## Read File name with index from File_data DF\n","        a.loc[i,'File name'] = df_wikipedia['filename'][index] ## Read File name with index from File_data DF\n","    for j,simScore in enumerate(linear_similarities[:-11:-1]):\n","        a.loc[j,'Score'] = simScore\n","    return a\n","\n","#SearchDocument('johnson and johnson vaccines derived from murdered babies. at least 2 other vaccines are not. if you take vaccines use those.')\n","\n","#SearchDocument('Michael Yeadon\tis a former person that was important to someone')\n","\n","SearchDocument('Together with collaborators, we founded a spinout company to develop and commercialise a nasal spray vaccine against COVID. The vaccine is based on research by our and @UniEastFinland researchers. #HelsinkiHealth #corona #COVID19 #covidvaccine')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"ynI7j319WqHB","executionInfo":{"status":"ok","timestamp":1663847721275,"user_tz":-120,"elapsed":10098,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"0c2325fd-7528-40e5-bebf-58c7d7558e92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max Sequence Length: 512\n"]},{"output_type":"execute_result","data":{"text/plain":["  index                                            Content  \\\n","0   139  sinds januari 2020 wordt er door meerdere orga...   \n","1   766   protocol mrna 1273 p301 pdf . moderna. protoc...   \n","2   746  a universal coronavirus vaccine is effective a...   \n","3   823  curevac n.v. is a german biopharmaceutical com...   \n","4   824  in 2007 curevac received the innovation prize ...   \n","5   764  in june 2021 a report revealed that the ub 612...   \n","6   827  cvncov is an mrna vaccine that encodes a minim...   \n","7   745  intranasal vaccines target mucosal immunity in...   \n","8   177  one idea used to support a laboratory origin i...   \n","9   305  international research on vaccines and medicin...   \n","\n","                 File name     Score  \n","0                 COVID-19  0.497134  \n","1         COVID-19_vaccine  0.496551  \n","2         COVID-19_vaccine  0.490719  \n","3                  CureVac  0.490441  \n","4                  CureVac  0.478521  \n","5         COVID-19_vaccine  0.463849  \n","6                  CureVac  0.461640  \n","7         COVID-19_vaccine  0.458389  \n","8  COVID-19_misinformation  0.442386  \n","9              COVID-19-en  0.433409  "],"text/html":["\n","  <div id=\"df-36d917da-c26e-4e5b-aaa9-cf1660e90ece\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Content</th>\n","      <th>File name</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>139</td>\n","      <td>sinds januari 2020 wordt er door meerdere orga...</td>\n","      <td>COVID-19</td>\n","      <td>0.497134</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>766</td>\n","      <td>protocol mrna 1273 p301 pdf . moderna. protoc...</td>\n","      <td>COVID-19_vaccine</td>\n","      <td>0.496551</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>746</td>\n","      <td>a universal coronavirus vaccine is effective a...</td>\n","      <td>COVID-19_vaccine</td>\n","      <td>0.490719</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>823</td>\n","      <td>curevac n.v. is a german biopharmaceutical com...</td>\n","      <td>CureVac</td>\n","      <td>0.490441</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>824</td>\n","      <td>in 2007 curevac received the innovation prize ...</td>\n","      <td>CureVac</td>\n","      <td>0.478521</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>764</td>\n","      <td>in june 2021 a report revealed that the ub 612...</td>\n","      <td>COVID-19_vaccine</td>\n","      <td>0.463849</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>827</td>\n","      <td>cvncov is an mrna vaccine that encodes a minim...</td>\n","      <td>CureVac</td>\n","      <td>0.461640</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>745</td>\n","      <td>intranasal vaccines target mucosal immunity in...</td>\n","      <td>COVID-19_vaccine</td>\n","      <td>0.458389</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>177</td>\n","      <td>one idea used to support a laboratory origin i...</td>\n","      <td>COVID-19_misinformation</td>\n","      <td>0.442386</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>305</td>\n","      <td>international research on vaccines and medicin...</td>\n","      <td>COVID-19-en</td>\n","      <td>0.433409</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36d917da-c26e-4e5b-aaa9-cf1660e90ece')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-36d917da-c26e-4e5b-aaa9-cf1660e90ece button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-36d917da-c26e-4e5b-aaa9-cf1660e90ece');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","source":["## Scoring"],"metadata":{"id":"Cpqt1YgkJPFg"}},{"cell_type":"markdown","source":["### Import CovidCheck"],"metadata":{"id":"TGGHCs6MJbW2"}},{"cell_type":"code","source":["import math\n","\n","covid_check = pd.read_csv('no_headings_nl_merged_clef_emnlp_downsampled_to_en - Sheet1.csv', sep=',')\n","covid_check['tokenized_covidcheck'] = covid_check['verify_text']\n","\n","covid_check['tokenized_covidcheck']= covid_check['tokenized_covidcheck'].str.lower()\n","SW=['<span>', '<li>', '</li>', '<ul>', '</ul>', '<b>', '</b>', '<h4>', '</h4>', '<h3>', '</h3>', '<h2>', '</h2>', '<h1>', '</h1>', '</link>', '</span>', '</i>']\n","for sw in SW:\n","    covid_check.tokenized_covidcheck=covid_check.tokenized_covidcheck.replace(to_replace=sw,value='',regex=True)\n","\n","print(type(covid_check['tokenized_covidcheck'][0]))\n","print(covid_check.tokenized_covidcheck[0])\n","#covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<[^<]+?>',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<p(.*?)>',value='<p>',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<link(.*?)>',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<span(.*?)>',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<i(.*?)>',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='[!\"#$%&\\'()*+,:;=?@[\\\\]^_`{|}~]',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='-',value=' ',regex=True)\n","#covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='\\s+',value=' ',regex=True)\n","#covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='\\n',value=' ',regex=True)\n","#covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='  ',value='',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.apply(lambda x:x.strip() if type(x) == str else x)\n","\n","covid_check[\"tokenized_covidcheck\"] = covid_check[\"tokenized_covidcheck\"].str.split(\"<p>\")\n","covid_check = covid_check.explode(\"tokenized_covidcheck\").reset_index(drop=True)\n","covid_check = covid_check[covid_check.tokenized_covidcheck != ' ']\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='<[^<]+?>',value=' ',regex=True)\n","covid_check.tokenized_covidcheck =covid_check.tokenized_covidcheck.replace(to_replace='  ',value=' ',regex=True)\n","covid_check[\"tokenized\"] = covid_check[\"tokenized_covidcheck\"]\n","\n","print(covid_check['tokenized_covidcheck'][27])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8gGWBLwVJdFY","executionInfo":{"status":"ok","timestamp":1663847085512,"user_tz":-120,"elapsed":1012,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"d2e5812b-c62a-4f21-fb61-6b98ec13f87d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'float'>\n","nan\n","nan\n"]}]},{"cell_type":"markdown","source":["### Final Scoring"],"metadata":{"id":"L5ExLe8bJf4q"}},{"cell_type":"code","source":["import scipy\n","import statistics\n","\n","index = 0\n","matches = 0\n","cosine_list = []\n","\n","def first_last_words(cur_list):\n","    fl_sentence = ''\n","    fl_list = []\n","    for sentence in cur_list:\n","      sent_list = sentence.split()\n","      if len(sent_list) > 3:\n","        fl_sentence = '{} {} {} {}'.format(sent_list[0], sent_list[1], sent_list[-2], sent_list[-1])\n","      elif len(sent_list) > 1:\n","        fl_sentence = sent_list[0]\n","      else:\n","        fl_sentence = None\n","      fl_list.append(fl_sentence)\n","    return fl_list\n","\n","def similarity_huggingface(query):\n","  # Calculatte similarity for huggingface models\n","  Q_Train = model.encode(q)\n","  linear_similarities = scipy.spatial.distance.cdist(Q_Train, embeddings, \"cosine\")[0]\n","  linear_similarities = 1 - linear_similarities\n","  return linear_similarities\n","\n","def similarity_tfhub(query):\n","  # Calculatte similarity for Tensorflow Hub models\n","  Q_Train =embed(query)\n","  #linear_similarities = linear_kernel(Q_Train, loadedmodel).flatten()\n","  linear_similarities = scipy.spatial.distance.cdist(Q_Train, loadedmodel, \"cosine\")[0]\n","  linear_similarities = 1 - linear_similarities\n","  return linear_similarities\n","\n","def tfidf_similarity():\n","  index = 0\n","  matches = 0\n","  for checkworthy in covid_check['class_label']:\n","    correct_docs = []\n","    if checkworthy == 1:\n","      # input check-worthy message\n","      query = covid_check['tweet_text'][index]\n","      preprocessed_query = re.sub(\"\\W+\", \" \", query).strip()\n","      tokens = word_tokenize(str(preprocessed_query))\n","\n","      q_df = pd.DataFrame(columns=['q_clean'])\n","      q_df.loc[0,'q_clean'] =tokens\n","      q_df['q_clean'] =wordLemmatizer(q_df.q_clean)\n","      q_df=q_df.replace(to_replace =\"\\[.\", value = '', regex = True)\n","      q_df=q_df.replace(to_replace =\"'\", value = '', regex = True)\n","      q_df=q_df.replace(to_replace =\" \", value = '', regex = True)\n","      q_df=q_df.replace(to_replace ='\\]', value = '', regex = True)\n","      \n","      # check if the correct document exists\n","      correct_text = ''\n","      if type(covid_check['verify_url'][index]) == str:\n","        correct_text = covid_check['tokenized_covidcheck'][index].split('\\n\\n')\n","        correct_text = [re.sub('\\s+', ' ', _) for _ in correct_text]\n","        correct_text = [re.sub('  ', '', _) for _ in correct_text]\n","      \n","      d_cosines = []\n","      query_vector = gen_vector_T(q_df['q_clean'])\n","      for d in tfidf_tran.A:\n","        d_cosines.append(cosine_sim(query_vector, d))\n","      # top 10 matches\n","      Top_index_doc = np.array(d_cosines).argsort()[-3:][::-1]\n","      print(len(Top_index_doc))\n","      # Check if retrieved top 10 contains correct document\n","      #print(\"top docs: \", Top_index_doc)\n","      predicted_content = [df_wikipedia['content'][index] for index in Top_index_doc]\n","      #print(predicted_docs)\n","      if max(np.array(d_cosines)) < 0.18:\n","        predicted_content = ['' for i in predicted_content]\n","\n","      predicted_content = first_last_words(predicted_content)\n","      correct_text = first_last_words(correct_text)\n","\n","      flat_cosines = np.array(d_cosines).flatten()\n","      flat_cosines.sort()\n","      flat_cosines = flat_cosines[-10:]\n","      cosine_list.extend(flat_cosines)\n","\n","      # Check if the correct answer is in the prediction\n","      if len(correct_text) > 0:\n","        for sentence in correct_text:\n","          if sentence in predicted_content:\n","            #print(len(correct_text))\n","            matches += 1\n","      else:\n","        if None in predicted_content:\n","          for i in predicted_content:\n","            matches += 1\n","    index += 1\n","  print(\"Amount correct: \", matches)\n","\n","# tfidf_similarity()\n","\n","for checkworthy in covid_check['class_label']:\n","  correct_text = []\n","  if checkworthy == 1:\n","    # input check-worthy message\n","    q =[covid_check['tweet_text'][index]]\n","    \n","    # check if the correct document exists\n","    if type(covid_check['tokenized_covidcheck'][index]) == str:\n","      correct_text = covid_check['tokenized_covidcheck'][index].split('\\n\\n')\n","      correct_text = [re.sub('\\s+', ' ', _) for _ in correct_text]\n","      correct_text = [re.sub('  ', '', _) for _ in correct_text]\n","    else:\n","      print(type(covid_check['tokenized_covidcheck'][index]))\n","\n","    linear_similarities = similarity_huggingface(q)\n","    # top 10 matches\n","    Top_index_doc = linear_similarities.argsort()[:-11:-1]\n","    # Check if retrieved top 10 contains correct document\n","    predicted_content = [df_wikipedia['content'][index] for index in Top_index_doc]\n","\n","    #print(max(linear_similarities))\n","    if max(linear_similarities) < 0.37:\n","      predicted_content = ['' for i in predicted_content]\n","      #print(predicted_content)\n","    #print(linear_similarities[:10])\n","\n","    # Check if the correct answer is in the prediction\n","    # print(correct_text)\n","    # print(predicted_content)\n","    # print('--------------')\n","\n","    linear_similarities.sort()\n","    linear_similarities = linear_similarities[-10:]\n","    cosine_list.extend(linear_similarities)\n","    predicted_content = first_last_words(predicted_content)\n","    correct_text = first_last_words(correct_text)\n","\n","    # Check if the correct answer is in the prediction\n","    if len(correct_text) > 0:\n","      for sentence in correct_text:\n","        if sentence in predicted_content:\n","          print('hoi')\n","          matches += 1\n","    else:\n","      if None in predicted_content:\n","        for i in predicted_content:\n","          matches += 1\n","  index += 1\n","\n","print(len(cosine_list))\n","print(statistics.mean(cosine_list))\n","print('matches: ',matches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvg-RXXMJRDl","executionInfo":{"status":"ok","timestamp":1663849061171,"user_tz":-120,"elapsed":828,"user":{"displayName":"Remi Thüss","userId":"00105744549434974572"}},"outputId":"fc548b14-7534-49d0-e099-af54f3246a73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'float'>\n","hoi\n","hoi\n","hoi\n","<class 'float'>\n","hoi\n","<class 'float'>\n","hoi\n","<class 'float'>\n","hoi\n","<class 'float'>\n","<class 'float'>\n","<class 'float'>\n","<class 'float'>\n","<class 'float'>\n","hoi\n","hoi\n","<class 'float'>\n","<class 'float'>\n","<class 'float'>\n","490\n","0.3697380503060291\n","matches:  58\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}
